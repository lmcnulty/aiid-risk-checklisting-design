# Algorithm Assessment Agnes

<img alt="Headshot of Agnes" src="./images/image2.png" width="300">

- **Age**: 35

- **Education**
  - BA in {{not CS or political science}}
    - took some math-heavy economics and research courses, intro CS
    - did an internship at a think tank working as an analyst.
  - Afterwards got an MS Data Science.

- **Professional Experience**
  - After getting her degree at 26, she spent three years 
    working as a data scientist at {{ some boring business }}

    **TODO**: Tone this down

    - That role -- and the entire team she was on --
      existed primarily so higher-ups
      could say that they're "data-driven",
      when in fact no one ever read any of the reports they produced
      or ran any of the models after the demo.

  - Worked at a startup coercing data 
    into looking sort of like a hockey stick.
    She left after just a year.

- **Organizational Home** & **Foundational experience in practice**

  For the past 6 years, Agnes has been Working at {{ audit company }}, 
  a [B Corp](https://en.wikipedia.org/wiki/Benefit_corporation)
<!-- ^^^^^^ Sort of a compromise between for-profit and non-profit -->
  with the mission of reducing AI harm
  by working with system developers 
  to identify and mitigate risks.

- **Engineering Capacities**:
  Knows Python, PyTorch, some visualization libraries,
  and data engineering stuff like Spark.
  Uses Jupyter Notebooks a lot.

- **Attentiveness to task**
  - If she chose to use the tool,
    then she's going to be highly attentive
    and play around a bunch with the parameters.

  - If she's pressured to use the tool by someone else,
    then she's going to be slightly--moderately inattentive,
    only going down the most obvious path of actions.

- **Customers**
  - ML Startups like Pymetrics who need 
    to develop trust in their core product.

  - Big companies that use AI somewhere in the business pipeline
    and want to advertise at responsible AI conferences.

- **Stakeholders**
  - The organization Agnes works for
  - The organization that they're auditing
    - The engineers of the system
    - The manager in charge of the project
  - The users and useds of the system
    - Some systems have "useds" in addition to users.
      For instance, in an automated system for hiring,
      the user is the hiring managers
      and the useds are the candidates.

- **Relationship with risks that are surfaced -- 
  what does she do with them now, what would she ideally do?**

  She writes a report detailing the problem,
  and its potential mitigations paths
  and gives it to somebody at the company, who skims it,
  then either...

  **TODO**: Tone this down

  - ...misrepresents what is it says
    to support doing whatever they planned to do anyway.

  - ...ignores the core issues it points out
    but maybe picks out one specific example
    intended to illustrate a general problem
    and changes some small details so that the specific example
    no longer occurs and they can say that they "fixed it."

  Ideally, she would be involved early enough
  that issues could be addressed in time 
  to make more than surface-level changes.


## References

- <https://cbw.sh/>

- [Building and Auditing Fair Algorithms: 
   A Case Study in Candidate Screening](
    https://evijit.github.io/docs/pymetrics_audit_FAccT.pdf
  )

- [Audit & Assurance Senior, AI/Algorithm – Deloitte](
    https://web.archive.org/web/20220819175429/https://apply.deloitte.com/careers/JobDetail/Audit-Assurance-Senior-AI-Algorithm/99280
  )
  
  > "Familiarity with AI and analytical software packages/tools/programming languages (e.g., SAS, R, Python, Spark, Hadoop Big Data, etc.)"

- <https://www.algorithmaudit.eu/>

- [Outsider Oversight:
  Designing a Third Party Audit Ecosystem for AI Governance](
    https://arxiv.org/pdf/2206.04737.pdf
  )

  > "**even well-executed algorithmic audit results may get co-opted by 
  >  audit targets and used to disguise deeper failures**. The 
  >  controversial facial recognition company Clearview AI, for 
  >  example, claimed to be "100% accurate across all demographic 
  >  groups according to the ACLU’s facial recognition accuracy 
  >  methodology" but failed to address the ethical and legal 
  >  challenges that accompany its sourcing of face data, which has 
  >  been subject to numerous legal actions (e.g., in California, 
  >  Illinois, the European Union)"

  > By convention, audits are classified into three types [29]. 
  > "First party" audits are conducted by a company of its own 
  > products. Many AI ethics teams can be conceived of as fulfilling 
  > a first-party audit role. "Second party" audits are performed by 
  > a contractual counterparty or an entity hired by that contractual 
  > counterparty. Second party audits typically ensure compliance 
  > with contract terms. As applied to AI, public sector procurement 
  > of AI products may require a second party audit to assess bias, 
  > as has been the case for legislative proposals to regulate facial 
  > recognition technology. "Third party" audits are conducted by 
  > ostensibly independent parties engaged specifically to conduct 
  > the audit, typically subject to pre-determined auditing standards.

- [Algorithmic Accountability Act of 2022](
    https://www.congress.gov/bill/117th-congress/house-bill/6580/text
  )
