
- [*Algorithm Assessment Agnes*](./personas/agnes.md) is auditing a startup called Pyrobotics

  - Pyrobotics makes robots for setting things on fire.

  - How does it work?
    They trained a classifier on millions of photos
    labeled by gig workers in Africa and the Philipines
    as either "should be set on fire" (e.g. firewood) or 
    "should not be set on fire" (e.g. humans).
    The robots are mounted with cameras and flamethrowers.
    The camera feed is fed into the classifier,
    and whenever it determines that something should be on fire,
    it activates the flamethrower.

  - Some people are worried that this might not be safe,
    so Pyrobotics has brought in Saiftee to audit their systems
    to show that they can be depended on.

- She's working on a report analyzing Pyrobotics's AI systems.

- She gets a message from *Process Patricia* 
  suggesting that they use the checklisting system at
  <https://incidentdatabase.ai/checklists/>.

- Agnes reads the first half of a short paragraph 
  explaining what the tool is.

  - Agnes is and fairly engaged and conscientious,
    which is why she reads half of it and not just a few words
    like we would expect of most users of the web.

- She looks below at the cloud of Goal tags 
  and then scrolls a bit to see 
  a similar cloud of Methodologies tags.
  She clicks some of the obvious ones.

  - These could be a bunch of badges,
    or perhaps a "word cloud" visualization
    where more common tags are larger.

- She clicks some of the incident titles
  but for now ignores the checklist
  -- she's doesn't feel ready to go through it properly.

- She clicks randomly through the site for a couple of minutes.

- She returns to her work analyzing the system,
  but in doing so, she now is also looking to find
  which of the tags would apply to Pyrobotics.

- At some point, Agnes returns to the checklist system
  Now has a good idea of which tags are applicable

- In a text box at the top of the page labeled "System Name"
  she enter "Pyrobotics."
  A skeleton appears below 
  while the app searches for matching incidents,
  but none are found.

- In the search box above the Goals tag cloud, she enters "fire",
  which filters all the tags except for "set things on fire."
  and "automated hiring and firing".
  
  She clicks "set things on fire".

- A skeleton starts pulsing below,
  as she selects "classification" from the tags cloud.

- A list of incidents appear below.
  The first is titled
  "IgnAIte's robot flamethrower fires on log cabin"

- She middle-click the headline
  and reads about how the former startup's system
  had not seen any log cabins in its training data
  and mistook it for firewood.

- She goes back to the checklist tab
  and sees a dropdown that allows her to choose "This risk is 
  { inapplicable | prevented | mitigated | unmitigated }"

- She selects "is not mitigated."

- The following elements appear below
  - Two sliders, one for "likelihood" and one "severity."

  - A text box for details on the risk and its lack of mitigation.

  - A text box for recommendations on how the specific risk 
    could be potentially mitigated.

  - A carousel showing incidents responses
    for incidents with matching goals, methods, and failures.

- She enters medium likelihood, high severity,
  and writes that system could misclassify an input
  not represented in its training data.

- She clicks through a couple of incident responses
  and reads those to find what other organizations did
  to mitigate similar harms.

  - She finds a report about a company that solved its fire problem
    with robots that sprayed water on anything
    they classified dangerous fires.
    She notes this in the recommendations text box.

- When each text box is filled, 
  a checkmark appears by the incident and it collapses.

- The remaining incidents don't present applicable risks,
  so she marks them "inapplicable."

- At the bottom of the page, there are two buttons
  - Save to database
  - Export (split button with default "PDF")

- She clicks "save to database"

- She's redirected to the sign up page.
  She makes an account
  and is redirected to the checklist page,
  where her results are still there.

- She clicks "save to database" again and a modal shows up.
  It warns her that content saved in the database
  will be available in public database dumps.
  There's a red "confirm" and a white "cancel" button.

- She clicks "cancel" since she's not sure if she's allowed 
  to make the details public.
  The modal closes.

- She clicks "export" and a PDF file
  showing the checklist formatted
  for inclusion in a report opens in a new tab.
  She saves this to a folder 
  where she keeps her materials on Pyrobotics.




